### Diff for 20250707_120413
diff --git a/DIFF_20250707_120413.md b/DIFF_20250707_120413.md
new file mode 100644
index 0000000..e6353db
--- /dev/null
+++ b/DIFF_20250707_120413.md
@@ -0,0 +1 @@
+### Diff for 20250707_120413
diff --git a/RECOMMENDATIONS_20250707_120413.md b/RECOMMENDATIONS_20250707_120413.md
new file mode 100644
index 0000000..d98bf4f
--- /dev/null
+++ b/RECOMMENDATIONS_20250707_120413.md
@@ -0,0 +1 @@
+### Recommendations 20250707_120413
diff --git a/example.env b/example.env
index daad136..1d8044c 100644
--- a/example.env
+++ b/example.env
@@ -6,6 +6,9 @@ API_KEY_GOOGLE=
 API_KEY_MISTRAL=
 API_KEY_OPENROUTER=
 API_KEY_SAMBANOVA=
+API_KEY_ANYTHINGLLM=
+API_KEY_TOGETHERAI=
+API_KEY_GROK=
 
 API_KEY_OPENAI_AZURE=
 OPENAI_AZURE_ENDPOINT=
@@ -23,6 +26,9 @@ LM_STUDIO_BASE_URL="http://127.0.0.1:1234/v1"
 LOCALAI_BASE_URL="http://127.0.0.1:8080/v1"
 OPEN_ROUTER_BASE_URL="https://openrouter.ai/api/v1"
 SAMBANOVA_BASE_URL="https://fast-api.snova.ai/v1"
+ANYTHINGLLM_BASE_URL="http://127.0.0.1:3001/v1"
+TOGETHERAI_BASE_URL="https://api.together.ai/v1"
+GROK_BASE_URL="https://api.grok.ai/v1"
 
 # Password for Remote Function Calls between instances
 RFC_PASSWORD=
diff --git a/models.py b/models.py
index 55404bf..1e85e29 100644
--- a/models.py
+++ b/models.py
@@ -51,6 +51,9 @@ class ModelProvider(Enum):
     LMSTUDIO = "LM Studio"
     LOCALAI = "LocalAI"
     MISTRALAI = "Mistral AI"
+    ANYTHINGLLM = "AnythingLLM"
+    TOGETHERAI = "Together AI"
+    GROK = "Grok"
     OLLAMA = "Ollama"
     OPENAI = "OpenAI"
     OPENAI_AZURE = "OpenAI Azure"
@@ -234,6 +237,101 @@ def get_localai_embedding(
     )  # type: ignore
 
 
+# AnythingLLM and other OpenAI compatible interfaces
+def get_anythingllm_base_url():
+    return (
+        dotenv.get_dotenv_value("ANYTHINGLLM_BASE_URL")
+        or f"http://{runtime.get_local_url()}:3001/v1"
+    )
+
+
+def get_anythingllm_chat(
+    model_name: str,
+    base_url=None,
+    api_key=None,
+    **kwargs,
+):
+    if not base_url:
+        base_url = get_anythingllm_base_url()
+    if not api_key:
+        api_key = get_api_key("anythingllm") or "none"
+    return ChatOpenAI(model_name=model_name, base_url=base_url, api_key=api_key, **kwargs)  # type: ignore
+
+
+def get_anythingllm_embedding(
+    model_name: str,
+    base_url=None,
+    api_key=None,
+    **kwargs,
+):
+    if not base_url:
+        base_url = get_anythingllm_base_url()
+    if not api_key:
+        api_key = get_api_key("anythingllm") or "none"
+    return OpenAIEmbeddings(model=model_name, api_key=api_key, base_url=base_url, check_embedding_ctx_length=False, **kwargs)  # type: ignore
+
+
+# TogetherAI and other OpenAI compatible interfaces
+def get_togetherai_base_url():
+    return (
+        dotenv.get_dotenv_value("TOGETHERAI_BASE_URL")
+        or "https://api.together.ai/v1"
+    )
+
+
+def get_togetherai_chat(
+    model_name: str,
+    base_url=None,
+    api_key=None,
+    **kwargs,
+):
+    if not base_url:
+        base_url = get_togetherai_base_url()
+    if not api_key:
+        api_key = get_api_key("togetherai")
+    return ChatOpenAI(model_name=model_name, api_key=api_key, base_url=base_url, **kwargs)  # type: ignore
+
+
+def get_togetherai_embedding(
+    model_name: str,
+    base_url=None,
+    api_key=None,
+    **kwargs,
+):
+    if not base_url:
+        base_url = get_togetherai_base_url()
+    if not api_key:
+        api_key = get_api_key("togetherai")
+    return OpenAIEmbeddings(model=model_name, api_key=api_key, base_url=base_url, **kwargs)  # type: ignore
+
+
+# Grok models
+def get_grok_chat(
+    model_name: str,
+    api_key=None,
+    base_url=None,
+    **kwargs,
+):
+    if not api_key:
+        api_key = get_api_key("grok")
+    if not base_url:
+        base_url = dotenv.get_dotenv_value("GROK_BASE_URL") or "https://api.grok.ai/v1"
+    return ChatOpenAI(api_key=api_key, model=model_name, base_url=base_url, **kwargs)  # type: ignore
+
+
+def get_grok_embedding(
+    model_name: str,
+    api_key=None,
+    base_url=None,
+    **kwargs,
+):
+    if not api_key:
+        api_key = get_api_key("grok")
+    if not base_url:
+        base_url = dotenv.get_dotenv_value("GROK_BASE_URL") or "https://api.grok.ai/v1"
+    return OpenAIEmbeddings(model=model_name, api_key=api_key, base_url=base_url, **kwargs)  # type: ignore
+
+
 # Anthropic models
 def get_anthropic_chat(
     model_name: str,
diff --git a/one_click_setup.sh b/one_click_setup.sh
index 432ec72..abf5242 100755
--- a/one_click_setup.sh
+++ b/one_click_setup.sh
@@ -1,16 +1,62 @@
 #!/bin/bash
-set -e
 
-# Install dependencies
-if [ -f requirements.txt ]; then
+set -euo pipefail
+
+# Print info messages
+log_info() {
+    echo "[INFO] $1"
+}
+
+log_error() {
+    echo "[ERROR] $1" >&2
+}
+
+# Catch errors and exit gracefully
+trap 'log_error "Setup failed at line $LINENO while running: $BASH_COMMAND"' ERR
+
+MCP_URL=""
+INSTALL_ELK=false
+
+# Parse custom arguments
+while [[ $# -gt 0 ]]; do
+    case "$1" in
+        --mcp-url)
+            MCP_URL="$2"
+            shift 2
+            ;;
+        --install-elk)
+            INSTALL_ELK=true
+            shift
+            ;;
+        *)
+            break
+            ;;
+    esac
+done
+
+log_info "Installing dependencies"
+if [[ -f requirements.txt ]]; then
+    python3 -m pip install --upgrade pip
     python3 -m pip install -r requirements.txt
 fi
 
-# Prepare environment
+# Optional ELK stack installation
+if [[ "$INSTALL_ELK" == true ]]; then
+    log_info "Installing ELK stack via scripts/install_elk.sh"
+    bash scripts/install_elk.sh || log_error "ELK installation failed"
+fi
+
+# Download MCP server if URL provided
+if [[ -n "$MCP_URL" ]]; then
+    log_info "Downloading MCP server from $MCP_URL"
+    bash scripts/download_mcp.sh "$MCP_URL"
+fi
+
+log_info "Preparing environment"
 python3 prepare.py "$@"
 
-# Preload models or data
+log_info "Preloading models or data"
 python3 preload.py "$@"
 
-# Start the Agent Zero web UI
+log_info "Starting the Agent Zero web UI"
 exec python3 run_ui.py "$@"
diff --git a/python/helpers/log_monitor.py b/python/helpers/log_monitor.py
new file mode 100644
index 0000000..52d4752
--- /dev/null
+++ b/python/helpers/log_monitor.py
@@ -0,0 +1,29 @@
+import logging
+import threading
+import time
+from pathlib import Path
+
+LOG_DIR = Path("logs")
+
+
+def _monitor(log_dir: Path, stop: threading.Event) -> None:
+    positions: dict[Path, int] = {}
+    log_dir.mkdir(exist_ok=True)
+    while not stop.is_set():
+        for path in log_dir.glob("*.log"):
+            last_pos = positions.get(path, 0)
+            if path.exists() and path.stat().st_size > last_pos:
+                with path.open() as f:
+                    f.seek(last_pos)
+                    for line in f:
+                        if any(w in line for w in ("ERROR", "Exception")):
+                            logging.error("Log monitor detected issue: %s", line.strip())
+                    positions[path] = f.tell()
+        time.sleep(2)
+
+
+def start_log_monitor() -> threading.Event:
+    stop = threading.Event()
+    thread = threading.Thread(target=_monitor, args=(LOG_DIR, stop), daemon=True)
+    thread.start()
+    return stop
diff --git a/scripts/download_mcp.sh b/scripts/download_mcp.sh
new file mode 100755
index 0000000..04724ff
--- /dev/null
+++ b/scripts/download_mcp.sh
@@ -0,0 +1,20 @@
+#!/bin/bash
+set -euo pipefail
+URL="$1"
+DEST="mcp_servers"
+mkdir -p "$DEST"
+
+if [[ "$URL" == *.git ]]; then
+    git clone "$URL" "$DEST/$(basename "$URL" .git)" || {
+        echo "Failed to clone $URL" >&2
+        exit 1
+    }
+else
+    filename=$(basename "$URL")
+    curl -L "$URL" -o "$DEST/$filename" || {
+        echo "Failed to download $URL" >&2
+        exit 1
+    }
+fi
+
+echo "MCP server downloaded to $DEST"
diff --git a/scripts/install_elk.sh b/scripts/install_elk.sh
new file mode 100755
index 0000000..e00a8ab
--- /dev/null
+++ b/scripts/install_elk.sh
@@ -0,0 +1,26 @@
+#!/bin/bash
+set -euo pipefail
+
+# Simple docker compose for ELK stack
+if [[ ! -d deploy/elk ]]; then
+    mkdir -p deploy/elk
+fi
+
+cat > deploy/elk/docker-compose.yml <<'YAML'
+version: '3'
+services:
+  elasticsearch:
+    image: docker.elastic.co/elasticsearch/elasticsearch:8.14.0
+    environment:
+      - discovery.type=single-node
+    ports:
+      - "9200:9200"
+  kibana:
+    image: docker.elastic.co/kibana/kibana:8.14.0
+    ports:
+      - "5601:5601"
+    depends_on:
+      - elasticsearch
+YAML
+
+docker compose -f deploy/elk/docker-compose.yml up -d
diff --git a/tests/test_anythingllm.py b/tests/test_anythingllm.py
new file mode 100644
index 0000000..31bea58
--- /dev/null
+++ b/tests/test_anythingllm.py
@@ -0,0 +1,5 @@
+from pathlib import Path
+
+def test_anythingllm_provider_present():
+    text = Path('models.py').read_text()
+    assert 'ANYTHINGLLM' in text
diff --git a/tests/test_grok.py b/tests/test_grok.py
new file mode 100644
index 0000000..d5f6083
--- /dev/null
+++ b/tests/test_grok.py
@@ -0,0 +1,5 @@
+from pathlib import Path
+
+def test_grok_provider_present():
+    text = Path('models.py').read_text()
+    assert 'GROK' in text
diff --git a/tests/test_togetherai.py b/tests/test_togetherai.py
new file mode 100644
index 0000000..b0ab518
--- /dev/null
+++ b/tests/test_togetherai.py
@@ -0,0 +1,5 @@
+from pathlib import Path
+
+def test_togetherai_provider_present():
+    text = Path('models.py').read_text()
+    assert 'TOGETHERAI' in text
