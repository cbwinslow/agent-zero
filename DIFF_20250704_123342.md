diff --git a/docs/README.md b/docs/README.md
index e4cc7b2..e7b7dc2 100644
--- a/docs/README.md
+++ b/docs/README.md
@@ -4,6 +4,7 @@ To begin with Agent Zero, follow the links below for detailed guides on various
 
 - **[Installation](installation.md):** Set up (or [update](installation.md#how-to-update-agent-zero)) Agent Zero on your system.
 - **[Usage Guide](usage.md):** Explore GUI features and usage scenarios.
+- **[Features & Workflows](features_workflows.md):** Overview of key capabilities and deployment steps.
 - **[Architecture Overview](architecture.md):** Understand the internal workings of the framework.
 - **[Contributing](contribution.md):** Learn how to contribute to the Agent Zero project.
 - **[Troubleshooting and FAQ](troubleshooting.md):** Find answers to common issues and questions.
diff --git a/docs/installation.md b/docs/installation.md
index bbe4636..0bdbf9f 100644
--- a/docs/installation.md
+++ b/docs/installation.md
@@ -272,6 +272,31 @@ Once you've downloaded some models, you might want to check which ones you have
 
 - Experiment with different model combinations to find the balance of performance and cost that best suits your needs. E.g., faster and lower latency LLMs will help, and you can also use `faiss_gpu` instead of `faiss_cpu` for the memory.
 
+## Installing and Using LocalAI
+LocalAI provides an OpenAI-compatible API for running models on your own hardware.
+
+### Installation
+1. Download a release from the [LocalAI GitHub repository](https://github.com/go-skynet/LocalAI).
+2. Extract the binary and start it with `./local-ai` (defaults to port `8080`).
+3. Optionally configure ports and model directories with `--config`.
+
+### Selecting LocalAI within Agent Zero
+1. Open the Settings page in the Web UI.
+2. Choose **LocalAI** as the provider for chat or embedding models.
+3. Enter the URL such as `http://localhost:8080/v1` and specify the model name.
+4. Click **Save**.
+
+```mermaid
+sequenceDiagram
+    actor U as User
+    participant AZ as Agent Zero
+    participant L as LocalAI
+    U->>AZ: Chat request
+    AZ->>L: OpenAI API call
+    L-->>AZ: Response
+    AZ-->>U: Result
+```
+
 ## Using Agent Zero on your mobile device
 Agent Zero's Web UI is accessible from any device on your network through the Docker container:
 
diff --git a/example.env b/example.env
index 739ba51..daad136 100644
--- a/example.env
+++ b/example.env
@@ -20,6 +20,7 @@ USE_CLOUDFLARE=false
 
 OLLAMA_BASE_URL="http://127.0.0.1:11434"
 LM_STUDIO_BASE_URL="http://127.0.0.1:1234/v1"
+LOCALAI_BASE_URL="http://127.0.0.1:8080/v1"
 OPEN_ROUTER_BASE_URL="https://openrouter.ai/api/v1"
 SAMBANOVA_BASE_URL="https://fast-api.snova.ai/v1"
 
diff --git a/models.py b/models.py
index ec270cb..55404bf 100644
--- a/models.py
+++ b/models.py
@@ -49,6 +49,7 @@ class ModelProvider(Enum):
     GROQ = "Groq"
     HUGGINGFACE = "HuggingFace"
     LMSTUDIO = "LM Studio"
+    LOCALAI = "LocalAI"
     MISTRALAI = "Mistral AI"
     OLLAMA = "Ollama"
     OPENAI = "OpenAI"
@@ -193,6 +194,46 @@ def get_lmstudio_embedding(
     return OpenAIEmbeddings(model=model_name, api_key="none", base_url=base_url, check_embedding_ctx_length=False, **kwargs)  # type: ignore
 
 
+# LocalAI and other OpenAI compatible interfaces
+def get_localai_base_url():
+    return (
+        dotenv.get_dotenv_value("LOCALAI_BASE_URL")
+        or f"http://{runtime.get_local_url()}:8080/v1"
+    )
+
+
+def get_localai_chat(
+    model_name: str,
+    base_url=None,
+    api_key=None,
+    **kwargs,
+):
+    if not base_url:
+        base_url = get_localai_base_url()
+    if not api_key:
+        api_key = get_api_key("localai") or "none"
+    return ChatOpenAI(model_name=model_name, base_url=base_url, api_key=api_key, **kwargs)  # type: ignore
+
+
+def get_localai_embedding(
+    model_name: str,
+    base_url=None,
+    api_key=None,
+    **kwargs,
+):
+    if not base_url:
+        base_url = get_localai_base_url()
+    if not api_key:
+        api_key = get_api_key("localai") or "none"
+    return OpenAIEmbeddings(
+        model=model_name,
+        api_key=api_key,
+        base_url=base_url,
+        check_embedding_ctx_length=False,
+        **kwargs,
+    )  # type: ignore
+
+
 # Anthropic models
 def get_anthropic_chat(
     model_name: str,
